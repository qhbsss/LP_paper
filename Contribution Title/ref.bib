@ARTICLE{b1,
  author={Khalil, Ruhul Amin and Jones, Edward and Babar, Mohammad Inayatullah and Jan, Tariqullah and Zafar, Mohammad Haseeb and Alhussain, Thamer},
  journal={IEEE Access}, 
  title={Speech Emotion Recognition Using Deep Learning Techniques: A Review}, 
  year={2019},
  volume={7},
  number={},
  pages={117327-117345},
  keywords={Databases;Emotion recognition;Feature extraction;Speech recognition;Deep learning;Human computer interaction;Hidden Markov models;Speech emotion recognition;deep learning;deep neural network;deep Boltzmann machine;recurrent neural network;deep belief network;convolutional neural network},
  doi={10.1109/ACCESS.2019.2936124}}

@INPROCEEDINGS{b2,
  author={Al-Ali, Mahra and Deriche, Mohamed and Derbel, Nabil},
  booktitle={2024 21st International Multi-Conference on Systems, Signals \& Devices (SSD)}, 
  title={An Improved Time-Frequency Based Deep Learning Algorithm for Speech Emotion Recognition}, 
  year={2024},
  volume={},
  number={},
  pages={334-339},
  keywords={Deep learning;Time-frequency analysis;Emotion recognition;Image segmentation;Speech recognition;Frequency estimation;Data models;Emotion Detection;Speech Emotion Recognition;Deep Learning;CNN;Traditional CNN;DenseNet;VGG;Spectrogram;Spectral Centroid;Pitch Frequency;Spectral Roll off;IEMOCAP},
  doi={10.1109/SSD61670.2024.10548773}}

@INPROCEEDINGS{b3,
  author={Yuan, Zhichen and Philip Chen, C. L. and Li, Shuzhen and Zhang, Tong},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Disentanglement Network: Disentangle the Emotional Features from Acoustic Features for Speech Emotion Recognition}, 
  year={2024},
  volume={},
  number={},
  pages={11686-11690},
  keywords={Human computer interaction;Emotion recognition;Speech recognition;Signal processing;Feature extraction;Acoustics;Speech processing;Speech emotion recognition;IEMOCAP;Emo-DB;HuBERT;Disentanglement},
  doi={10.1109/ICASSP48485.2024.10448044}}

@INPROCEEDINGS{b4,
  author={Chen, Weidong and Xing, Xiaofen and Xu, Xiangmin and Pang, Jianxin and Du, Lan},
  booktitle={ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={DST: Deformable Speech Transformer for Emotion Recognition}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  keywords={Deformable models;Emotion recognition;Adaptation models;Speech recognition;Signal processing;Transformers;Acoustics;speech emotion recognition;deformable network;Transformer;deformable attention mechanism},
  doi={10.1109/ICASSP49357.2023.10096966}}

@ARTICLE{b5,
  author={Ong, Kah Liang and Lee, Chin Poo and Lim, Heng Siong and Lim, Kian Ming and Alqahtani, Ali},
  journal={IEEE Access}, 
  title={Mel-MViTv2: Enhanced Speech Emotion Recognition With Mel Spectrogram and Improved Multiscale Vision Transformers}, 
  year={2023},
  volume={11},
  number={},
  pages={108571-108579},
  keywords={Speech recognition;Emotion recognition;Feature extraction;Spectrogram;Transformers;Mel frequency cepstral coefficient;Speech enhancement;Speech;speech emotion;speech emotion recognition;spectrogram;mel spectrogram;mel spectrogram with short-time Fourier transform;vision transformer;improved multiscale vision transformers;Emo-DB;RAVDESS;IEMOCAP},
  doi={10.1109/ACCESS.2023.3321122}}

@ARTICLE{b6,
  author={Zhang, Chenghao and Xue, Lei},
  journal={IEEE Access}, 
  title={Autoencoder With Emotion Embedding for Speech Emotion Recognition}, 
  year={2021},
  volume={9},
  number={},
  pages={51231-51241},
  keywords={Feature extraction;Speech recognition;Emotion recognition;Spectrogram;Noise reduction;Hidden Markov models;Acoustics;Speech emotion recognition;autoencoder;emotion embedding;instance normalization},
  doi={10.1109/ACCESS.2021.3069818}}

@INPROCEEDINGS{b7,
  author={Hashemi, Saber and Asgari, Mohammad},
  booktitle={2023 31st International Conference on Electrical Engineering (ICEE)}, 
  title={Vision Transformer and Parallel Convolutional Neural Network for Speech Emotion Recognition}, 
  year={2023},
  volume={},
  number={},
  pages={888-892},
  keywords={Electrical engineering;Emotion recognition;Computational modeling;Neural networks;Speech recognition;Feature extraction;Transformers;speech emotion recognition;vision transformer;convolutional neural network;attention mechanism},
  doi={10.1109/ICEE59167.2023.10334797}}

@INPROCEEDINGS{b8,
  author={Ye, Jiaxin and Wen, Xin-Cheng and Wei, Yujie and Xu, Yong and Liu, Kunhong and Shan, Hongming},
  booktitle={ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Temporal Modeling Matters: A Novel Temporal Emotional Modeling Approach for Speech Emotion Recognition}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  keywords={Adaptation models;Emotion recognition;Visualization;Fuses;Source coding;Speech recognition;Spatiotemporal phenomena;Speech emotion recognition;bi-direction;multi-scale;dynamic fusion;temporal modeling},
  doi={10.1109/ICASSP49357.2023.10096370}}


@ARTICLE{b9,
  author={Pan, Lei and Wang, Qi},
  journal={IEEE Access}, 
  title={GFRN-SEA: Global-Aware Feature Representation Network for Speech Emotion Analysis}, 
  year={2024},
  volume={12},
  number={},
  pages={161500-161518},
  keywords={Feature extraction;Spectrogram;Mel frequency cepstral coefficient;Data mining;Speech recognition;Acoustics;Emotion recognition;Convolutional neural networks;Robustness;Speech analysis;Multi-dimensional convolution;multi-scale convolution;multi-layer cross attention;speech emotion recognition;feature fusion},
  doi={10.1109/ACCESS.2024.3490186}}


@INPROCEEDINGS{b10,
  author={Georgescu, Alexandru-Lucian and Chivu, Gheorghe-Iulian and Cucu, Horia},
  booktitle={2024 15th International Conference on Communications (COMM)}, 
  title={Exploring Fusion Techniques for Multimodal Emotion Recognition}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  keywords={Emotion recognition;Visualization;Accuracy;Virtual assistants;Speech recognition;Mental health;Manuals;Machine learning;User experience;Speech processing;speech emotion recognition;visual emotion recognition;audio–visual emotion recognition;late fusion},
  doi={10.1109/COMM62355.2024.10741464}}


@INPROCEEDINGS{b11,
  author={Zhang, Chengwen and Zhang, Yuhao and Cheng, Bo},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={RL-EMO: A Reinforcement Learning Framework for Multimodal Emotion Recognition}, 
  year={2024},
  volume={},
  number={},
  pages={10246-10250},
  keywords={Emotion recognition;Codes;Convolution;Semantics;Reinforcement learning;Speech recognition;Acoustics;Reinforcement Learning;Multimodal Emotion Recognition},
  doi={10.1109/ICASSP48485.2024.10446459}}

@INPROCEEDINGS{b12,
  author={Zhao, Zihan and Wang, Yu and Wang, Yanfeng},
  booktitle={ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Knowledge-Aware Bayesian Co-Attention for Multimodal Emotion Recognition}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  keywords={Emotion recognition;Fuses;Speech recognition;Signal processing;Acoustics;Bayes methods;Speech processing;multimodal emotion recognition;transfer learning;Bayesian attention;knowledge injection},
  doi={10.1109/ICASSP49357.2023.10095798}}


@article{b13,
   title={WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing},
   volume={16},
   ISSN={1941-0484},
   url={http://dx.doi.org/10.1109/JSTSP.2022.3188113},
   DOI={10.1109/jstsp.2022.3188113},
   number={6},
   journal={IEEE Journal of Selected Topics in Signal Processing},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Chen, Sanyuan and Wang, Chengyi and Chen, Zhengyang and Wu, Yu and Liu, Shujie and Chen, Zhuo and Li, Jinyu and Kanda, Naoyuki and Yoshioka, Takuya and Xiao, Xiong and Wu, Jian and Zhou, Long and Ren, Shuo and Qian, Yanmin and Qian, Yao and Wu, Jian and Zeng, Michael and Yu, Xiangzhan and Wei, Furu},
   year={2022},
   month=oct, pages={1505–1518} }


@article{b18,
  author       = {Mingxing Tan and
                  Quoc V. Le},
  title        = {EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},
  journal      = {CoRR},
  volume       = {abs/1905.11946},
  year         = {2019},
  url          = {http://arxiv.org/abs/1905.11946},
  eprinttype    = {arXiv},
  eprint       = {1905.11946},
  timestamp    = {Mon, 03 Jun 2019 13:42:33 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1905-11946.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@INPROCEEDINGS{b14,
  author={He, Yurun and Minematsu, Nobuaki and Saito, Daisuke},
  booktitle={ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Multiple Acoustic Features Speech Emotion Recognition Using Cross-Attention Transformer}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  keywords={Emotion recognition;Fuses;Speech recognition;Transformers;Acoustics;Proposals;Task analysis;speech emotion recognition;transformer;cross attention;multi features fusion},
  doi={10.1109/ICASSP49357.2023.10095777}}


@INPROCEEDINGS{b15,
  author={Liu, Ke and Wang, Dekui and Wu, Dongya and Feng, Jun},
  booktitle={ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Speech Emotion Recognition Via Two-Stream Pooling Attention With Discriminative Channel Weighting}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  keywords={Representation learning;Emotion recognition;Fuses;Convolution;Speech recognition;Acoustics;Task analysis;Discriminative feature weighting;feature fusion;attention;speech emotion recognition},
  doi={10.1109/ICASSP49357.2023.10095588}}


@INPROCEEDINGS{b16,
  author={Jiao, Xinxin and Wang, Liejun and Yu, Yinfeng},
  booktitle={2024 IEEE International Conference on Multimedia and Expo (ICME)}, 
  title={MFHCA: Enhancing Speech Emotion Recognition Via Multi-Spatial Fusion and Hierarchical Cooperative Attention}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  keywords={Human computer interaction;Emotion recognition;Accuracy;Speech recognition;Speech enhancement;Feature extraction;Acoustics;Speech emotion recognition;multi-spatial fusion;hierarchical cooperative attention;Hubert features},
  doi={10.1109/ICME57554.2024.10688053}}

@misc{b17,
      title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces}, 
      author={Albert Gu and Tri Dao},
      year={2024},
      eprint={2312.00752},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2312.00752}, 
}
%mamba

@INPROCEEDINGS{b19,
  author={Zou, Heqing and Si, Yuke and Chen, Chen and Rajan, Deepu and Chng, Eng Siong},
  booktitle={ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Speech Emotion Recognition with Co-Attention Based Multi-Level Acoustic Information}, 
  year={2022},
  volume={},
  number={},
  pages={7367-7371},
  keywords={Emotion recognition;Speech recognition;Feature extraction;Acoustics;Data mining;Speech processing;Task analysis;Speech emotion recognition;Multimodal fusion;Multi-level acoustic information;Co-attention mechanism},
  doi={10.1109/ICASSP43922.2022.9747095}}

@Article{b20,
author={Busso, Carlos
and Bulut, Murtaza
and Lee, Chi-Chun
and Kazemzadeh, Abe
and Mower, Emily
and Kim, Samuel
and Chang, Jeannette N.
and Lee, Sungbok
and Narayanan, Shrikanth S.},
title={IEMOCAP: interactive emotional dyadic motion capture database},
journal={Language Resources and Evaluation},
year={2008},
month={Dec},
day={01},
volume={42},
number={4},
pages={335-359},
abstract={Since emotions are expressed through a combination of verbal and non-verbal channels, a joint analysis of speech and gestures is required to understand expressive human communication. To facilitate such investigations, this paper describes a new corpus named the 鈥渋nteractive emotional dyadic motion capture database鈥? (IEMOCAP), collected by the Speech Analysis and Interpretation Laboratory (SAIL) at the University of Southern California (USC). This database was recorded from ten actors in dyadic sessions with markers on the face, head, and hands, which provide detailed information about their facial expressions and hand movements during scripted and spontaneous spoken communication scenarios. The actors performed selected emotional scripts and also improvised hypothetical scenarios designed to elicit specific types of emotions (happiness, anger, sadness, frustration and neutral state). The corpus contains approximately 12聽h of data. The detailed motion capture information, the interactive setting to elicit authentic emotions, and the size of the database make this corpus a valuable addition to the existing databases in the community for the study and modeling of multimodal and expressive human communication.},
issn={1574-0218},
doi={10.1007/s10579-008-9076-6},
url={https://doi.org/10.1007/s10579-008-9076-6}
}
@inproceedings{b21,
  title     = {Self-Attention for Speech Emotion Recognition},
  author    = {Lorenzo Tarantino and Philip N. Garner and Alexandros Lazaridis},
  year      = {2019},
  booktitle = {Interspeech 2019},
  pages     = {2578--2582},
  doi       = {10.21437/Interspeech.2019-2822},
  issn      = {2958-1796},
}
@article{b22,
  title={Speech emotion recognition based on Graph-LSTM neural network},
  author={Li, Yan and Wang, Yapeng and Yang, Xu and Im, Sio-Kei},
  journal={EURASIP Journal on Audio, Speech, and Music Processing},
  volume={2023},
  number={1},
  pages={40},
  year={2023},
  publisher={Springer}
}